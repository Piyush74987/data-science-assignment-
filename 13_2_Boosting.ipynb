{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6369af8a-9466-4a35-9e76-354b5dab4626",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression (GBR) is a machine learning algorithm that belongs to the family of boosting algorithms. It is a supervised learning algorithm used for both regression and classification problems.\n",
    "\n",
    "In Gradient Boosting Regression (GBR), a decision tree is fitted to the data in a step-wise manner, with each new tree attempting to correct the errors made by the previous tree. The prediction of the ensemble model is obtained by summing the predictions of all the individual trees.\n",
    "\n",
    "The \"gradient\" in gradient boosting refers to the use of the gradient descent optimization algorithm to minimize the loss function (e.g., mean squared error) of the model. At each step, the gradient of the loss function with respect to the prediction is computed, and the tree is fitted to the negative gradient, which corresponds to the direction of steepest descent.\n",
    "\n",
    "GBR is a powerful algorithm that can handle a large number of features and nonlinear relationships between the features and the target variable. However, it can be prone to overfitting if the model is too complex or if the data is noisy. Therefore, it is important to tune the hyperparameters of the model (e.g., the number of trees, the learning rate, the maximum depth of the trees) to achieve the best performance on the validation set.\n",
    "\n",
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f764f6db-2c33-46b9-943b-b84bc86b6cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 2998.32\n",
      "R-squared: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.intercept = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.intercept = np.mean(y)\n",
    "        residual = y - self.intercept\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            self.trees.append(tree)\n",
    "            pred = tree.predict(X)\n",
    "            residual -= self.learning_rate * pred\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return self.intercept + self.learning_rate * np.sum(preds, axis=0)\n",
    "\n",
    "# Generate a random regression problem\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.5)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "n_train = int(0.8 * len(X))\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_test, y_test = X[n_train:], y[n_train:]\n",
    "\n",
    "# Train a gradient boosting regressor on the training set\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = gb.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean squared error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cd679c-76bf-447b-9731-94a2fb7db4cb",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "\n",
    "Here is an implementation of grid search to find the best hyperparameters for the GradientBoostingRegressor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82d43d-637f-46eb-b439-a4672f8706b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import arange\n",
    "\n",
    "# Generate a random regression problem\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.5)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "n_train = int(0.8 * len(X))\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_test, y_test = X[n_train:], y[n_train:]\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [2, 3, 4],\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor object\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Use GridSearchCV to search for the best hyperparameters\n",
    "grid_search = GridSearchCV(gb, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean squared error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d600fcf-e5de-4dd5-94a1-3e7fe580932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this implementation, we use scikit-learn's GridSearchCV to search for the best hyperparameters. We can define a dictionary of hyperparameters to search over, and then pass this dictionary and the GradientBoostingRegressor object to GridSearchCV. We also specify a 5-fold cross-validation to evaluate the models.\n",
    "\n",
    "You can also use RandomizedSearchCV to perform a random search over the hyperparameter space. Here's an example of how to use RandomizedSearchCV:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
